\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumerate}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}

\newcommand{\eps}{\varepsilon}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\R}{\mathbb{R}}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf ECE-GY 9223 Reinforcement Learning} \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Name: #4}{ #1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex

\begin{document}

\lecture{Reading Project Proposal}{Spring 2019}{Prof.\ Quanyan Zhu}{Yunian Pan}

\section{Introduction}
 
To generate more scalable algorithms with higher efficiency and fewer open parameters, reinforcement learning (RL) has moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory.  In this vein, $ \cite{1}$ suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-JacobiBellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. 
 
In the spirit of these latter ideas, this paper addresses a new method of probabilistic reinforcement learning derived from the framework of stochastic optimal control and path integrals.

Theoretical development, practical applications and important characteristics are presented in this paper, as well as related work and some main issues addressed.

\section{Motivation}

Besides the paper has been cited 413 times, I was motivated by the fact that it generated a new stochastic version of classical method framework using statistical estimation theory, and there's enough mathematical substances in this paper, which is exciting to readers. 

\section{My Plan}

\begin{enumerate}[(a)]
\item Feb.16th $\to$ Mar.1st: 2 weeks for reading and theoretical derivation, 
\item Mar.2nd $\to$ Mar.8th: 1 week for implementation and simulation, 
\item Mar.8th $\to$ Mar.14th: 1 week for report writing and presentation.
\end{enumerate}

\bibliographystyle{alpha}

\begin{thebibliography}{42}

\bibitem{1}
Theodorou, Evangelos, Jonas Buchli, and Stefan Schaal. "A generalized path integral control approach to reinforcement learning." journal of machine learning research 11.Nov (2010): 3137-3181.	


\end{thebibliography}

\end{document}
